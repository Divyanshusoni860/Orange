{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16bfb628",
   "metadata": {},
   "source": [
    "Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e631362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a1cf2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://BlackPanther:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ml</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x12594fe4670>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ml\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f726450",
   "metadata": {},
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0872d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(longitude=-122.23, latitude=37.88, housing_median_age=41, total_rooms=880, total_bedrooms=129, population=322, households=126, median_income=8.3252, median_house_value=452600, ocean_proximity='NEAR BAY'),\n",
       " Row(longitude=-122.22, latitude=37.86, housing_median_age=21, total_rooms=7099, total_bedrooms=1106, population=2401, households=1138, median_income=8.3014, median_house_value=358500, ocean_proximity='NEAR BAY'),\n",
       " Row(longitude=-122.24, latitude=37.85, housing_median_age=52, total_rooms=1467, total_bedrooms=190, population=496, households=177, median_income=7.2574, median_house_value=352100, ocean_proximity='NEAR BAY'),\n",
       " Row(longitude=-122.25, latitude=37.85, housing_median_age=52, total_rooms=1274, total_bedrooms=235, population=558, households=219, median_income=5.6431, median_house_value=341300, ocean_proximity='NEAR BAY'),\n",
       " Row(longitude=-122.25, latitude=37.85, housing_median_age=52, total_rooms=1627, total_bedrooms=280, population=565, households=259, median_income=3.8462, median_house_value=342200, ocean_proximity='NEAR BAY')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"../data/housing.csv\",header=True,inferSchema=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c189295",
   "metadata": {},
   "source": [
    "About Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f436133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: integer (nullable = true)\n",
      " |-- total_rooms: integer (nullable = true)\n",
      " |-- total_bedrooms: integer (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      " |-- households: integer (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: integer (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ccc9534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude',\n",
       " 'latitude',\n",
       " 'housing_median_age',\n",
       " 'total_rooms',\n",
       " 'total_bedrooms',\n",
       " 'population',\n",
       " 'households',\n",
       " 'median_income',\n",
       " 'median_house_value',\n",
       " 'ocean_proximity']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee9dfe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+-------------------+\n",
      "|longitude_nulls|latitude_nulls|median_income_nulls|\n",
      "+---------------+--------------+-------------------+\n",
      "|              0|             0|                  0|\n",
      "+---------------+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, isnan\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "numeric_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, (DoubleType, FloatType))]\n",
    "\n",
    "df.select([\n",
    "    sum((col(c).isNull() | isnan(col(c))).cast(\"int\")).alias(c + \"_nulls\")\n",
    "    for c in numeric_cols\n",
    "]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d19a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income'],  # replace with your feature columns\n",
    "    outputCol=\"Independent_Features\",\n",
    "    handleInvalid=\"skip\"  # very important!\n",
    ")\n",
    "\n",
    "final_data = assembler.transform(df).select(\"Independent_Features\", \"median_house_value\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b8eced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = featureassembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8fb6c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|Independent Features|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
      "|  -122.23|   37.88|                41|        880|           129|       322|       126|       8.3252|            452600|       NEAR BAY|[-122.23,37.88,41...|\n",
      "|  -122.22|   37.86|                21|       7099|          1106|      2401|      1138|       8.3014|            358500|       NEAR BAY|[-122.22,37.86,21...|\n",
      "|  -122.24|   37.85|                52|       1467|           190|       496|       177|       7.2574|            352100|       NEAR BAY|[-122.24,37.85,52...|\n",
      "|  -122.25|   37.85|                52|       1274|           235|       558|       219|       5.6431|            341300|       NEAR BAY|[-122.25,37.85,52...|\n",
      "|  -122.25|   37.85|                52|       1627|           280|       565|       259|       3.8462|            342200|       NEAR BAY|[-122.25,37.85,52...|\n",
      "|  -122.25|   37.85|                52|        919|           213|       413|       193|       4.0368|            269700|       NEAR BAY|[-122.25,37.85,52...|\n",
      "|  -122.25|   37.84|                52|       2535|           489|      1094|       514|       3.6591|            299200|       NEAR BAY|[-122.25,37.84,52...|\n",
      "|  -122.25|   37.84|                52|       3104|           687|      1157|       647|         3.12|            241400|       NEAR BAY|[-122.25,37.84,52...|\n",
      "|  -122.26|   37.84|                42|       2555|           665|      1206|       595|       2.0804|            226700|       NEAR BAY|[-122.26,37.84,42...|\n",
      "|  -122.25|   37.84|                52|       3549|           707|      1551|       714|       3.6912|            261100|       NEAR BAY|[-122.25,37.84,52...|\n",
      "|  -122.26|   37.85|                52|       2202|           434|       910|       402|       3.2031|            281500|       NEAR BAY|[-122.26,37.85,52...|\n",
      "|  -122.26|   37.85|                52|       3503|           752|      1504|       734|       3.2705|            241800|       NEAR BAY|[-122.26,37.85,52...|\n",
      "|  -122.26|   37.85|                52|       2491|           474|      1098|       468|        3.075|            213500|       NEAR BAY|[-122.26,37.85,52...|\n",
      "|  -122.26|   37.84|                52|        696|           191|       345|       174|       2.6736|            191300|       NEAR BAY|[-122.26,37.84,52...|\n",
      "|  -122.26|   37.85|                52|       2643|           626|      1212|       620|       1.9167|            159200|       NEAR BAY|[-122.26,37.85,52...|\n",
      "|  -122.26|   37.85|                50|       1120|           283|       697|       264|        2.125|            140000|       NEAR BAY|[-122.26,37.85,50...|\n",
      "|  -122.27|   37.85|                52|       1966|           347|       793|       331|        2.775|            152500|       NEAR BAY|[-122.27,37.85,52...|\n",
      "|  -122.27|   37.85|                52|       1228|           293|       648|       303|       2.1202|            155500|       NEAR BAY|[-122.27,37.85,52...|\n",
      "|  -122.26|   37.84|                50|       2239|           455|       990|       419|       1.9911|            158700|       NEAR BAY|[-122.26,37.84,50...|\n",
      "|  -122.27|   37.84|                52|       1503|           298|       690|       275|       2.6033|            162900|       NEAR BAY|[-122.27,37.84,52...|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9b9c6586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude',\n",
       " 'latitude',\n",
       " 'housing_median_age',\n",
       " 'total_rooms',\n",
       " 'total_bedrooms',\n",
       " 'population',\n",
       " 'households',\n",
       " 'median_income',\n",
       " 'median_house_value',\n",
       " 'ocean_proximity',\n",
       " 'Independent Features']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "96938fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-07-10 22:55:07.695\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Independent_Features` cannot be resolved. Did you mean one of the following? [`Independent Features`, `latitude`, `longitude`, `median_income`, `median_house_value`]. SQLSTATE: 42703\", \"context\": {\"file\": \"jdk.internal.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o839.select.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Independent_Features` cannot be resolved. Did you mean one of the following? [`Independent Features`, `latitude`, `longitude`, `median_income`, `median_house_value`]. SQLSTATE: 42703;\\n'Project ['Independent_Features, median_house_value#430]\\n+- Project [longitude#422, latitude#423, housing_median_age#424, total_rooms#425, total_bedrooms#426, population#427, households#428, median_income#429, median_house_value#430, ocean_proximity#431, UDF(struct(longitude, longitude#422, latitude, latitude#423, housing_median_age_double_VectorAssembler_db7d53ddb3f4, cast(housing_median_age#424 as double), total_rooms_double_VectorAssembler_db7d53ddb3f4, cast(total_rooms#425 as double), total_bedrooms_double_VectorAssembler_db7d53ddb3f4, cast(total_bedrooms#426 as double), population_double_VectorAssembler_db7d53ddb3f4, cast(population#427 as double), households_double_VectorAssembler_db7d53ddb3f4, cast(households#428 as double), median_income, median_income#429)) AS Independent Features#1106]\\n   +- Relation [longitude#422,latitude#423,housing_median_age#424,total_rooms#425,total_bedrooms#426,population#427,households#428,median_income#429,median_house_value#430,ocean_proximity#431] csv\\n\\r\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\r\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\r\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\r\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\\r\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\r\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\r\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\r\\n\\tat scala.util.Try$.apply(Try.scala:217)\\r\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\r\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\r\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\r\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\r\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\r\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\r\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:894)\\r\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:232)\\r\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\r\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\r\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\r\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\\r\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\r\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\r\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\r\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\r\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\r\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\r\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\r\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\r\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\r\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\r\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\r\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\r\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\r\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\r\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\r\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\r\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\r\\n\\t\\t... 21 more\\r\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"d:\\\\Anaconda\\\\envs\\\\tf-env\\\\lib\\\\site-packages\\\\pyspark\\\\errors\\\\exceptions\\\\captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"d:\\\\Anaconda\\\\envs\\\\tf-env\\\\lib\\\\site-packages\\\\py4j\\\\protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Independent_Features` cannot be resolved. Did you mean one of the following? [`Independent Features`, `latitude`, `longitude`, `median_income`, `median_house_value`]. SQLSTATE: 42703;\n'Project ['Independent_Features, median_house_value#430]\n+- Project [longitude#422, latitude#423, housing_median_age#424, total_rooms#425, total_bedrooms#426, population#427, households#428, median_income#429, median_house_value#430, ocean_proximity#431, UDF(struct(longitude, longitude#422, latitude, latitude#423, housing_median_age_double_VectorAssembler_db7d53ddb3f4, cast(housing_median_age#424 as double), total_rooms_double_VectorAssembler_db7d53ddb3f4, cast(total_rooms#425 as double), total_bedrooms_double_VectorAssembler_db7d53ddb3f4, cast(total_bedrooms#426 as double), population_double_VectorAssembler_db7d53ddb3f4, cast(population#427 as double), households_double_VectorAssembler_db7d53ddb3f4, cast(households#428 as double), median_income, median_income#429)) AS Independent Features#1106]\n   +- Relation [longitude#422,latitude#423,housing_median_age#424,total_rooms#425,total_bedrooms#426,population#427,households#428,median_income#429,median_house_value#430,ocean_proximity#431] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m final_data \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIndependent_Features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedian_house_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\tf-env\\lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:991\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParentDataFrame:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m--> 991\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\tf-env\\lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\tf-env\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `Independent_Features` cannot be resolved. Did you mean one of the following? [`Independent Features`, `latitude`, `longitude`, `median_income`, `median_house_value`]. SQLSTATE: 42703;\n'Project ['Independent_Features, median_house_value#430]\n+- Project [longitude#422, latitude#423, housing_median_age#424, total_rooms#425, total_bedrooms#426, population#427, households#428, median_income#429, median_house_value#430, ocean_proximity#431, UDF(struct(longitude, longitude#422, latitude, latitude#423, housing_median_age_double_VectorAssembler_db7d53ddb3f4, cast(housing_median_age#424 as double), total_rooms_double_VectorAssembler_db7d53ddb3f4, cast(total_rooms#425 as double), total_bedrooms_double_VectorAssembler_db7d53ddb3f4, cast(total_bedrooms#426 as double), population_double_VectorAssembler_db7d53ddb3f4, cast(population#427 as double), households_double_VectorAssembler_db7d53ddb3f4, cast(households#428 as double), median_income, median_income#429)) AS Independent Features#1106]\n   +- Relation [longitude#422,latitude#423,housing_median_age#424,total_rooms#425,total_bedrooms#426,population#427,households#428,median_income#429,median_house_value#430,ocean_proximity#431] csv\n"
     ]
    }
   ],
   "source": [
    "final_data = output.select([\"Independent_Features\",\"median_house_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c90ffb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+------------------------+-----------------+--------------------+----------------+----------------+-------------------+------------------------+---------------------+\n",
      "|longitude_nulls|latitude_nulls|housing_median_age_nulls|total_rooms_nulls|total_bedrooms_nulls|population_nulls|households_nulls|median_income_nulls|median_house_value_nulls|ocean_proximity_nulls|\n",
      "+---------------+--------------+------------------------+-----------------+--------------------+----------------+----------------+-------------------+------------------------+---------------------+\n",
      "|              0|             0|                       0|                0|                 207|               0|               0|                  0|                       0|                    0|\n",
      "+---------------+--------------+------------------------+-----------------+--------------------+----------------+----------------+-------------------+------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c + \"_nulls\") \n",
    "    for c in df.columns\n",
    "]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9dbad543",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63f1b73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|Independent_Features|median_house_value|\n",
      "+--------------------+------------------+\n",
      "|[-122.23,37.88,41...|            452600|\n",
      "|[-122.22,37.86,21...|            358500|\n",
      "|[-122.24,37.85,52...|            352100|\n",
      "|[-122.25,37.85,52...|            341300|\n",
      "|[-122.25,37.85,52...|            342200|\n",
      "|[-122.25,37.85,52...|            269700|\n",
      "|[-122.25,37.84,52...|            299200|\n",
      "|[-122.25,37.84,52...|            241400|\n",
      "|[-122.26,37.84,42...|            226700|\n",
      "|[-122.25,37.84,52...|            261100|\n",
      "|[-122.26,37.85,52...|            281500|\n",
      "|[-122.26,37.85,52...|            241800|\n",
      "|[-122.26,37.85,52...|            213500|\n",
      "|[-122.26,37.84,52...|            191300|\n",
      "|[-122.26,37.85,52...|            159200|\n",
      "|[-122.26,37.85,50...|            140000|\n",
      "|[-122.27,37.85,52...|            152500|\n",
      "|[-122.27,37.85,52...|            155500|\n",
      "|[-122.26,37.84,50...|            158700|\n",
      "|[-122.27,37.84,52...|            162900|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7555e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "train_data,test_data=final_data.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "31dc0ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+\n",
      "|Independent_Features                                  |\n",
      "+------------------------------------------------------+\n",
      "|[-124.35,40.54,52.0,1820.0,300.0,806.0,270.0,3.0147]  |\n",
      "|[-124.3,41.8,19.0,2672.0,552.0,1298.0,478.0,1.9797]   |\n",
      "|[-124.27,40.69,36.0,2349.0,528.0,1194.0,465.0,2.5179] |\n",
      "|[-124.25,40.28,32.0,1430.0,419.0,434.0,187.0,1.9417]  |\n",
      "|[-124.23,40.81,52.0,1112.0,209.0,544.0,172.0,3.3462]  |\n",
      "|[-124.23,41.75,11.0,3159.0,616.0,1343.0,479.0,2.4805] |\n",
      "|[-124.21,40.75,32.0,1218.0,331.0,620.0,268.0,1.6528]  |\n",
      "|[-124.21,41.75,20.0,3810.0,787.0,1993.0,721.0,2.0074] |\n",
      "|[-124.21,41.77,17.0,3461.0,722.0,1947.0,647.0,2.5795] |\n",
      "|[-124.19,40.73,21.0,5694.0,1056.0,2907.0,972.0,3.5363]|\n",
      "|[-124.19,40.77,30.0,2975.0,634.0,1367.0,583.0,2.442]  |\n",
      "|[-124.19,40.78,37.0,1371.0,319.0,640.0,260.0,1.8242]  |\n",
      "|[-124.18,40.62,35.0,952.0,178.0,480.0,179.0,3.0536]   |\n",
      "|[-124.18,40.78,33.0,1076.0,222.0,656.0,236.0,2.5096]  |\n",
      "|[-124.18,40.78,34.0,1592.0,364.0,950.0,317.0,2.1607]  |\n",
      "|[-124.18,40.78,37.0,1453.0,293.0,867.0,310.0,2.5536]  |\n",
      "|[-124.18,40.79,40.0,1398.0,311.0,788.0,279.0,1.4668]  |\n",
      "|[-124.17,40.74,17.0,2026.0,338.0,873.0,313.0,4.0357]  |\n",
      "|[-124.17,40.75,13.0,2171.0,339.0,951.0,353.0,4.8516]  |\n",
      "|[-124.17,40.76,26.0,1776.0,361.0,992.0,380.0,2.8056]  |\n",
      "+------------------------------------------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "train_data.select(\"Independent_Features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7e0c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regressor=LinearRegression(featuresCol=\"Independent_Features\",labelCol=\"median_house_value\")\n",
    "regressor=regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "02cd3cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-42713.1397, -42632.3502, 1126.7549, -7.5118, 103.9766, -44.4211, 68.7357, 39736.1689])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a1739f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 71257.23594610339\n",
      "R2: 0.6216297900955414\n"
     ]
    }
   ],
   "source": [
    "results = regressor.evaluate(test_data)\n",
    "print(\"RMSE:\", results.rootMeanSquaredError)\n",
    "print(\"R2:\", results.r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5fa66053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-------------------+\n",
      "|Independent_Features|median_house_value|         prediction|\n",
      "+--------------------+------------------+-------------------+\n",
      "|[-124.3,41.84,17....|            103600| 101139.99501619674|\n",
      "|[-124.26,40.58,52...|            111400| 163995.05224909494|\n",
      "|[-124.23,40.54,52...|            106700| 189372.95931623224|\n",
      "|[-124.22,41.73,28...|             78300|  77912.77150168642|\n",
      "|[-124.19,41.78,15...|             74600|  52854.62374744238|\n",
      "|[-124.18,40.79,39...|             70500| 110016.42117540073|\n",
      "|[-124.17,40.62,32...|             86400|  155755.6242909059|\n",
      "|[-124.17,41.76,20...|            105900|  82753.28608818026|\n",
      "|[-124.16,40.79,46...|             90600| 147781.34851425933|\n",
      "|[-124.16,40.8,52....|             80500| 150344.06006699335|\n",
      "|[-124.15,40.79,37...|             86400| 158281.04776625987|\n",
      "|[-124.15,41.81,17...|            103100|-21375.832810187712|\n",
      "|[-124.14,40.57,29...|             75100| 136316.59162206436|\n",
      "|[-124.14,40.78,35...|             92800| 175943.36997744162|\n",
      "|[-124.14,40.8,32....|             72600| 127671.81309328089|\n",
      "|[-124.14,41.95,21...|            122400|  73026.18088861136|\n",
      "|[-124.13,40.79,32...|             92800| 174271.09360144986|\n",
      "|[-124.11,40.95,19...|             81700| 129166.40717036743|\n",
      "|[-124.1,40.95,17....|             78400|    84251.492588263|\n",
      "|[-124.09,40.88,26...|             82100|   121848.141570949|\n",
      "+--------------------+------------------+-------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "pred_results=regressor.evaluate(test_data)\n",
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "93df01ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
Tunne likha kya lavde itna sab 😹
